{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7022eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Mini-Project: Contrastive Goal-Conditioned RL with TD3 + HER + Auto-Curriculum\n",
    "\n",
    "This is a SINGLE-FILE reference implementation designed to run on any Gymnasium\n",
    "GoalEnv-compatible task (dict observations with keys: 'observation',\n",
    "'achieved_goal', 'desired_goal'). It has been tested primarily with\n",
    "`gymnasium-robotics`' FetchPickAndPlace-v2 (MuJoCo) but the code is generic.\n",
    "\n",
    "Core features:\n",
    "- TD3 agent (actor-critic, target networks, policy delay, target policy smoothing)\n",
    "- HER replay buffer (future strategy)\n",
    "- Contrastive encoder f(s), f(g) trained with InfoNCE on replay to produce\n",
    "  goal-aware embeddings; policy consumes [f(s) || f(g) || proprio]\n",
    "- Optional dense shaping from state-goal L2 (can be toggled)\n",
    "- Auto-curriculum that adjusts a scalar difficulty by calling an optional\n",
    "  `set_difficulty(float)` method on the underlying env if available; otherwise\n",
    "  it no-ops but still tracks/prints success-based difficulty.\n",
    "\n",
    "Dependencies:\n",
    "  pip install gymnasium gymnasium-robotics torch numpy\n",
    "\n",
    "Example run:\n",
    "  python gcrl_td3_her_contrastive_curriculum.py --env FetchPickAndPlace-v2 --steps 2_000_00 \\\n",
    "      --contrastive --auto_curriculum --dense_reward\n",
    "\n",
    "Notes:\n",
    "- If your environment isn't a GoalEnv, wrap it or adapt the observation parsing\n",
    "  in `split_obs()`.\n",
    "- If your env supports curriculum, expose `env.unwrapped.set_difficulty(x: float)`.\n",
    "  This script will call it when success rate crosses thresholds.\n",
    "- Keep this as a learning/reference script; for paper-quality results, move the\n",
    "  classes into modules and add proper logging/ckpting.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import collections\n",
    "import dataclasses\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env ENV] [--steps STEPS]\n",
      "                             [--start_steps START_STEPS]\n",
      "                             [--update_after UPDATE_AFTER]\n",
      "                             [--update_every UPDATE_EVERY]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--buffer_size BUFFER_SIZE] [--lr LR]\n",
      "                             [--gamma GAMMA] [--tau TAU]\n",
      "                             [--policy_noise POLICY_NOISE]\n",
      "                             [--noise_clip NOISE_CLIP]\n",
      "                             [--policy_freq POLICY_FREQ]\n",
      "                             [--expl_noise EXPL_NOISE] [--cpu] [--seed SEED]\n",
      "                             [--contrastive] [--emb_dim EMB_DIM]\n",
      "                             [--enc_lr ENC_LR] [--n_negatives N_NEGATIVES]\n",
      "                             [--temperature TEMPERATURE] [--dense_reward]\n",
      "                             [--auto_curriculum] [--cur_window CUR_WINDOW]\n",
      "                             [--eval_every EVAL_EVERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1000/jupyter/runtime/kernel-v34aa4470d8aa48873344f6efb4ce9cf2bf74dc4d8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoth/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Utils --------------------\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def mlp(sizes, act=nn.ReLU, out_act=nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        if i < len(sizes) - 2:\n",
    "            layers.append(act())\n",
    "        else:\n",
    "            layers.append(out_act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Networks --------------------\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_dim: int, act_dim: int, max_action: float, hidden=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp([in_dim, *hidden, act_dim])\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.net(x)) * self.max_action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim: int, act_dim: int, hidden=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.q1 = mlp([in_dim + act_dim, *hidden, 1])\n",
    "        self.q2 = mlp([in_dim + act_dim, *hidden, 1])\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        xa = torch.cat([x, a], dim=-1)\n",
    "        return self.q1(xa), self.q2(xa)\n",
    "\n",
    "    def q1_only(self, x, a):\n",
    "        xa = torch.cat([x, a], dim=-1)\n",
    "        return self.q1(xa)\n",
    "\n",
    "\n",
    "# -------------------- Contrastive Encoder --------------------\n",
    "\n",
    "class ContrastiveEncoder(nn.Module):\n",
    "    \"\"\"Encodes state and goal vectors into a shared embedding space.\n",
    "\n",
    "    Use simple MLPs with shared trunk or Siamese-style weight sharing.\n",
    "    We treat inputs as flat vectors (e.g., proprio+positions). If you have images,\n",
    "    replace with a Conv encoder and feed flattened features here.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim_state: int, in_dim_goal: int, emb_dim: int = 64, hidden=(128, 128)):\n",
    "        super().__init__()\n",
    "        # Separate encoders allow asymmetric preprocessing if needed.\n",
    "        self.state_enc = mlp([in_dim_state, *hidden, emb_dim])\n",
    "        self.goal_enc = mlp([in_dim_goal, *hidden, emb_dim])\n",
    "\n",
    "    def encode_state(self, s):\n",
    "        z = self.state_enc(s)\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "    def encode_goal(self, g):\n",
    "\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "    def info_nce_loss(self, z_s: torch.Tensor, z_g_pos: torch.Tensor, z_g_neg: torch.Tensor, temperature: float = 0.07):\n",
    "        \"\"\"InfoNCE with multiple negatives per state.\n",
    "        z_s: [B, D], z_g_pos: [B, D], z_g_neg: [B, K, D]\n",
    "        \"\"\"\n",
    "        B, D = z_s.shape\n",
    "        K = z_g_neg.shape[1]\n",
    "        # logits: [B, 1+K]\n",
    "        pos = torch.sum(z_s * z_g_pos, dim=-1, keepdim=True) / temperature\n",
    "        neg = torch.bmm(z_g_neg, z_s.unsqueeze(-1)).squeeze(-1) / temperature  # [B, K]\n",
    "        logits = torch.cat([pos, neg], dim=1)\n",
    "        labels = torch.zeros(B, dtype=torch.long, device=z_s.device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# -------------------- HER Replay Buffer --------------------\n",
    "\n",
    "Transition = collections.namedtuple(\n",
    "    \"Transition\",\n",
    "    [\"obs\", \"ag\", \"g\", \"action\", \"reward\", \"obs_next\", \"ag_next\", \"done\"],\n",
    ")\n",
    "\n",
    "class HERBuffer:\n",
    "    def __init__(self, obs_dim, ag_dim, g_dim, act_dim, size=int(1e6), her_k=4, future_p=0.8, device=\"cpu\"):\n",
    "        self.size = size\n",
    "        self.her_k = her_k\n",
    "        self.future_p = future_p\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        # store as numpy for memory\n",
    "        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.ag = np.zeros((size, ag_dim), dtype=np.float32)\n",
    "        self.g = np.zeros((size, g_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.obs_next = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.ag_next = np.zeros((size, ag_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((size, 1), dtype=np.float32)\n",
    "        # episode indices to support future relabeling\n",
    "        self.ep_start_idxs: List[int] = []\n",
    "        self.ep_lengths: List[int] = []\n",
    "        self._current_ep_len = 0\n",
    "\n",
    "    def start_episode(self):\n",
    "        if self._current_ep_len > 0:\n",
    "            self.ep_lengths.append(self._current_ep_len)\n",
    "        self.ep_start_idxs.append(self.ptr)\n",
    "        self._current_ep_len = 0\n",
    "\n",
    "    def end_episode(self):\n",
    "        if self._current_ep_len > 0:\n",
    "            self.ep_lengths.append(self._current_ep_len)\n",
    "            self._current_ep_len = 0\n",
    "\n",
    "    def store(self, obs, ag, g, action, reward, obs_next, ag_next, done):\n",
    "        idx = self.ptr\n",
    "        self.obs[idx] = obs\n",
    "        self.ag[idx] = ag\n",
    "        self.g[idx] = g\n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = reward\n",
    "        self.obs_next[idx] = obs_next\n",
    "        self.ag_next[idx] = ag_next\n",
    "        self.dones[idx] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.size\n",
    "        self.full = self.full or self.ptr == 0\n",
    "        self._current_ep_len += 1\n",
    "\n",
    "    def _sample_idxs(self, batch_size):\n",
    "        max_idx = self.size if self.full else self.ptr\n",
    "        idxs = np.random.randint(0, max_idx, size=batch_size)\n",
    "        return idxs\n",
    "\n",
    "    def sample(self, batch_size, her_ratio=0.8, reward_fn=None):\n",
    "        idxs = self._sample_idxs(batch_size)\n",
    "        obs = self.obs[idxs].copy()\n",
    "        ag = self.ag[idxs].copy()\n",
    "        g = self.g[idxs].copy()\n",
    "        actions = self.actions[idxs].copy()\n",
    "        obs_next = self.obs_next[idxs].copy()\n",
    "        ag_next = self.ag_next[idxs].copy()\n",
    "        dones = self.dones[idxs].copy()\n",
    "\n",
    "        # HER relabeling with future strategy\n",
    "        her_mask = np.random.rand(batch_size) < her_ratio\n",
    "        for i, use_her in enumerate(her_mask):\n",
    "            if not use_her:\n",
    "                continue\n",
    "            # choose a future transition within same episode\n",
    "            idx = idxs[i]\n",
    "            # find episode bounds containing idx\n",
    "            # naive scan (okay for clarity); for scale, precompute mapping\n",
    "            ep_start, ep_end = None, None\n",
    "            for s, l in zip(self.ep_start_idxs, self.ep_lengths):\n",
    "                if s <= idx < s + l:\n",
    "                    ep_start, ep_end = s, s + l\n",
    "                    break\n",
    "            if ep_start is None:\n",
    "                continue\n",
    "            future_idx = np.random.randint(idx, ep_end)\n",
    "            g[i] = self.ag_next[future_idx]\n",
    "            # recompute reward if fn given\n",
    "        if reward_fn is not None:\n",
    "            rewards = reward_fn(ag_next, g)\n",
    "        else:\n",
    "            rewards = self.rewards[idxs].copy()\n",
    "\n",
    "        # to torch\n",
    "        device = self.device\n",
    "        to_t = lambda x: torch.as_tensor(x, device=device, dtype=torch.float32)\n",
    "        batch = {\n",
    "            'obs': to_t(obs), 'ag': to_t(ag), 'g': to_t(g), 'actions': to_t(actions),\n",
    "            'obs_next': to_t(obs_next), 'ag_next': to_t(ag_next), 'dones': to_t(dones),\n",
    "            'rewards': to_t(rewards)\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "# -------------------- TD3 Agent --------------------\n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, state_dim, action_dim, max_action, lr=1e-3, gamma=0.98, tau=0.005,\n",
    "                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, device=\"cpu\"):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.total_it = 0\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state: np.ndarray, noise_std: float = 0.1):\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state_t).cpu().numpy()[0]\n",
    "        if noise_std > 0:\n",
    "            action = action + np.random.normal(0, noise_std, size=action.shape)\n",
    "        return np.clip(action, -self.max_action, self.max_action)\n",
    "\n",
    "    def train(self, replay: HERBuffer, batch_size: int, reward_fn=None, actor_input_is_emb=True):\n",
    "        self.total_it += 1\n",
    "        batch = replay.sample(batch_size, reward_fn=reward_fn)\n",
    "        # Construct policy inputs: either raw [obs||g] or embeddings already provided\n",
    "        # In this single-file design, we assume upstream concatenation was done and passed in 'obs'\n",
    "        # so we use obs as state input for actor/critic.\n",
    "        state = batch['obs']\n",
    "        next_state = batch['obs_next']\n",
    "        action = batch['actions']\n",
    "        reward = batch['rewards']\n",
    "        done = batch['dones']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target = reward + (1 - done) * self.gamma * target_q\n",
    "\n",
    "        # Critic update\n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_q1, target) + F.mse_loss(current_q2, target)\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # Delayed actor update\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            actor_loss = -self.critic.q1_only(state, self.actor(state)).mean()\n",
    "            self.actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_opt.step()\n",
    "\n",
    "            # Soft update\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    p_targ.data.mul_(1 - self.tau)\n",
    "                    p_targ.data.add_(self.tau * p.data)\n",
    "                for p, p_targ in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    p_targ.data.mul_(1 - self.tau)\n",
    "                    p_targ.data.add_(self.tau * p.data)\n",
    "\n",
    "        return critic_loss.item()\n",
    "\n",
    "\n",
    "# -------------------- Environment helpers --------------------\n",
    "\n",
    "def split_obs(obs_dict: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split GoalEnv dict observation into flat vectors.\n",
    "    Returns (obs, achieved_goal, desired_goal) all as 1D np arrays.\n",
    "    \"\"\"\n",
    "    return obs_dict[\"observation\"].astype(np.float32), obs_dict[\"achieved_goal\"].astype(np.float32), obs_dict[\"desired_goal\"].astype(np.float32)\n",
    "\n",
    "\n",
    "def default_reward_fn(ag_next: np.ndarray, g: np.ndarray) -> np.ndarray:\n",
    "    # Negative distance + success bonus can be implemented in env; here we keep simple dense shaping\n",
    "    d = np.linalg.norm(ag_next - g, axis=-1, keepdims=True)\n",
    "    return -d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067757af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------- Auto-Curriculum --------------------\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Curriculum:\n",
    "    target_high: float = 0.7\n",
    "    target_low: float = 0.3\n",
    "    difficulty: float = 0.2  # [0,1]\n",
    "    step: float = 0.05\n",
    "    window: int = 100\n",
    "    history: collections.deque = dataclasses.field(default_factory=lambda: collections.deque(maxlen=100))\n",
    "\n",
    "    def update(self, success: bool, env=None):\n",
    "        self.history.append(1.0 if success else 0.0)\n",
    "        if len(self.history) < max(10, self.history.maxlen // 5):\n",
    "            return self.difficulty\n",
    "        rate = np.mean(self.history)\n",
    "        if rate > self.target_high:\n",
    "            self.difficulty = min(1.0, self.difficulty + self.step)\n",
    "        elif rate < self.target_low:\n",
    "            self.difficulty = max(0.0, self.difficulty - self.step)\n",
    "        # Try to notify env if it supports difficulty\n",
    "        if env is not None and hasattr(env.unwrapped, \"set_difficulty\"):\n",
    "            try:\n",
    "                env.unwrapped.set_difficulty(float(self.difficulty))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return self.difficulty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "\n",
    "def train(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    env = gym.make(args.env)\n",
    "    eval_env = gym.make(args.env)\n",
    "\n",
    "    assert isinstance(env.observation_space, gym.spaces.Dict), \"Env must be GoalEnv-like with Dict observations\"\n",
    "\n",
    "    obs0, _ = env.reset(seed=args.seed)\n",
    "    o, ag, g = split_obs(obs0)\n",
    "\n",
    "    obs_dim = o.shape[0]\n",
    "    ag_dim = ag.shape[0]\n",
    "    g_dim = g.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "\n",
    "    # Contrastive encoder\n",
    "    enc = None\n",
    "    emb_dim = 0\n",
    "    if args.contrastive:\n",
    "        enc = ContrastiveEncoder(in_dim_state=obs_dim + ag_dim, in_dim_goal=g_dim, emb_dim=args.emb_dim).to(device)\n",
    "        enc_opt = torch.optim.Adam(enc.parameters(), lr=args.enc_lr)\n",
    "        emb_dim = args.emb_dim\n",
    "\n",
    "    # TD3 over embedding input\n",
    "    policy_in_dim = (emb_dim * 2 + obs_dim) if args.contrastive else (obs_dim + g_dim)\n",
    "    agent = TD3(policy_in_dim, act_dim, max_action, lr=args.lr, gamma=args.gamma, tau=args.tau,\n",
    "                policy_noise=args.policy_noise, noise_clip=args.noise_clip, policy_freq=args.policy_freq, device=device)\n",
    "\n",
    "    # Replay\n",
    "    replay = HERBuffer(obs_dim=policy_in_dim, ag_dim=ag_dim, g_dim=g_dim, act_dim=act_dim, size=args.buffer_size,\n",
    "                       her_k=args.her_k, future_p=args.future_p, device=device)\n",
    "\n",
    "    # Curriculum\n",
    "    curriculum = Curriculum(window=args.cur_window)\n",
    "\n",
    "    # For reward shaping\n",
    "    if args.dense_reward:\n",
    "        reward_fn = default_reward_fn\n",
    "    else:\n",
    "        reward_fn = None\n",
    "\n",
    "    ep_ret, ep_len, ep_success = 0.0, 0, False\n",
    "    success_stats = collections.deque(maxlen=100)\n",
    "\n",
    "    # Start first episode in buffer (for episode-boundaries used by HER)\n",
    "    replay.start_episode()\n",
    "\n",
    "    obs_dict, _ = env.reset(seed=args.seed)\n",
    "    o, ag, g = split_obs(obs_dict)\n",
    "\n",
    "    # Helper to build policy state input\n",
    "    def build_state(o_vec, ag_vec, g_vec):\n",
    "        o_t = torch.as_tensor(o_vec, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        ag_t = torch.as_tensor(ag_vec, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        g_t = torch.as_tensor(g_vec, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        if args.contrastive:\n",
    "            with torch.no_grad():\n",
    "                z_s = enc.encode_state(torch.cat([o_t, ag_t], dim=-1))\n",
    "                z_g = enc.encode_goal(g_t)\n",
    "            state = torch.cat([z_s, z_g, o_t], dim=-1).squeeze(0).cpu().numpy()\n",
    "        else:\n",
    "            state = np.concatenate([o_vec, g_vec], axis=-1)\n",
    "        return state\n",
    "\n",
    "    state = build_state(o, ag, g)\n",
    "\n",
    "    # Storage for contrastive negatives\n",
    "    goal_bank = collections.deque(maxlen=10_000)\n",
    "\n",
    "    for t in range(1, args.steps + 1):\n",
    "        # Select action\n",
    "        if t < args.start_steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state, noise_std=args.expl_noise)\n",
    "\n",
    "        # Step\n",
    "        obs_next_dict, _, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        o2, ag2, g2 = split_obs(obs_next_dict)\n",
    "\n",
    "        # Compute reward if dense shaping is on (for buffer training); env may also return sparse reward via info\n",
    "        if args.dense_reward:\n",
    "            r = float(-np.linalg.norm(ag2 - g2))\n",
    "        else:\n",
    "            # try to read env's reward function via compute_reward if available\n",
    "            if hasattr(env.unwrapped, 'compute_reward'):\n",
    "                r = float(env.unwrapped.compute_reward(ag2, g2, info))\n",
    "            else:\n",
    "                r = 0.0\n",
    "\n",
    "        state_next = build_state(o2, ag2, g2)\n",
    "\n",
    "        # Store transition\n",
    "        replay.store(state, ag, g, action, r, state_next, ag2, float(done))\n",
    "\n",
    "        ep_len += 1\n",
    "        ep_ret += r\n",
    "\n",
    "        # success flag if provided\n",
    "        if 'is_success' in info:\n",
    "            ep_success = bool(info['is_success'])\n",
    "\n",
    "        # Move on\n",
    "        state = state_next\n",
    "        o, ag, g = o2, ag2, g2\n",
    "\n",
    "        if done:\n",
    "            success_stats.append(1.0 if ep_success else 0.0)\n",
    "            if args.auto_curriculum:\n",
    "                curriculum.update(ep_success, env)\n",
    "            # reset\n",
    "            obs_dict, _ = env.reset()\n",
    "            o, ag, g = split_obs(obs_dict)\n",
    "            state = build_state(o, ag, g)\n",
    "            replay.end_episode()\n",
    "            replay.start_episode()\n",
    "            ep_ret, ep_len, ep_success = 0.0, 0, False\n",
    "\n",
    "        # Fill negative goal bank\n",
    "        if len(goal_bank) < goal_bank.maxlen:\n",
    "            goal_bank.append(g.copy())\n",
    "\n",
    "        # Updates\n",
    "        if t >= args.update_after and t % args.update_every == 0:\n",
    "            for _ in range(args.update_every):\n",
    "                # Contrastive pretrain / joint train\n",
    "                if args.contrastive:\n",
    "                    batch = replay.sample(args.batch_size, reward_fn=reward_fn)\n",
    "                    # Build positives: (s: [o||ag], g: goal used in that transition)\n",
    "                    s_pos = torch.cat([batch['obs'][:, -obs_dim:], batch['ag']], dim=-1)  # Recover o from tail\n",
    "                    g_pos = batch['g']\n",
    "                    z_s = enc.encode_state(s_pos)\n",
    "                    z_g_pos = enc.encode_goal(g_pos)\n",
    "                    # Negatives: sample K negatives per state\n",
    "                    K = args.n_negatives\n",
    "                    with torch.no_grad():\n",
    "                        neg_goals = []\n",
    "                        for _ in range(K):\n",
    "                            idxs = np.random.randint(0, len(goal_bank), size=s_pos.shape[0])\n",
    "                            g_neg_np = np.stack([goal_bank[i] for i in idxs], axis=0)\n",
    "                            neg_goals.append(torch.as_tensor(g_neg_np, device=device, dtype=torch.float32))\n",
    "                        z_g_neg_list = [enc.encode_goal(g_neg) for g_neg in neg_goals]\n",
    "                        z_g_neg = torch.stack(z_g_neg_list, dim=1)  # [B,K,D]\n",
    "                    loss_c = enc.info_nce_loss(z_s, z_g_pos, z_g_neg, temperature=args.temperature)\n",
    "                    enc_opt.zero_grad()\n",
    "                    loss_c.backward()\n",
    "                    enc_opt.step()\n",
    "\n",
    "                # TD3 update\n",
    "                agent.train(replay, batch_size=args.batch_size, reward_fn=reward_fn)\n",
    "\n",
    "        # Periodic eval\n",
    "        if t % args.eval_every == 0:\n",
    "            sr = np.mean(success_stats) if len(success_stats) else 0.0\n",
    "            diff = curriculum.difficulty if args.auto_curriculum else None\n",
    "            print(f\"Step {t:>8} | recent SR={sr:.2f} | diff={diff} | replay_ptr={replay.ptr} | device={device}\")\n",
    "\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "\n",
    "\n",
    "# -------------------- CLI --------------------\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--env', type=str, default='FetchPickAndPlace-v2', help='Any GoalEnv-like env id')\n",
    "    p.add_argument('--steps', type=int, default=200_000)\n",
    "    p.add_argument('--start_steps', type=int, default=5000)\n",
    "    p.add_argument('--update_after', type=int, default=5000)\n",
    "    p.add_argument('--update_every', type=int, default=50)\n",
    "    p.add_argument('--batch_size', type=int, default=256)\n",
    "    p.add_argument('--buffer_size', type=int, default=1_000_000)\n",
    "    p.add_argument('--lr', type=float, default=1e-3)\n",
    "    p.add_argument('--gamma', type=float, default=0.98)\n",
    "    p.add_argument('--tau', type=float, default=0.005)\n",
    "    p.add_argument('--policy_noise', type=float, default=0.2)\n",
    "    p.add_argument('--noise_clip', type=float, default=0.5)\n",
    "    p.add_argument('--policy_freq', type=int, default=2)\n",
    "    p.add_argument('--expl_noise', type=float, default=0.1)\n",
    "    p.add_argument('--cpu', action='store_true')\n",
    "    p.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "    # Contrastive\n",
    "    p.add_argument('--contrastive', action='store_true')\n",
    "    p.add_argument('--emb_dim', type=int, default=64)\n",
    "    p.add_argument('--enc_lr', type=float, default=1e-3)\n",
    "    p.add_argument('--n_negatives', type=int, default=16)\n",
    "    p.add_argument('--temperature', type=float, default=0.07)\n",
    "\n",
    "    # Reward shaping\n",
    "    p.add_argument('--dense_reward', action='store_true')\n",
    "\n",
    "    # Auto-curriculum\n",
    "    p.add_argument('--auto_curriculum', action='store_true')\n",
    "    p.add_argument('--cur_window', type=int, default=100)\n",
    "\n",
    "    # Eval/prints\n",
    "    p.add_argument('--eval_every', type=int, default=5000)\n",
    "\n",
    "    return p.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    train(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
